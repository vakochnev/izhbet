#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ statistics_optimized.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ outcomes –∏ predictions –≤ –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
"""

import logging
import sys
from datetime import datetime
from typing import Dict, Any, List, Tuple
from tqdm import tqdm

from config import Session_pool
from db.models.outcome import Outcome
from db.models.prediction import Prediction
from db.models.match import Match
from db.models.championship import ChampionShip
from db.models.sport import Sport
# from db.models.statistics_optimized import StatisticsOptimized

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('migration_statistics.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class StatisticsDataMigrator:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ statistics_optimized."""
    
    def __init__(self, batch_size: int = 1000):
        self.batch_size = batch_size
        self.logger = logging.getLogger(__name__)
        
        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
        self.feature_mapping = {
            1: ('win_draw_loss', 'classification'),
            2: ('oz', 'classification'),
            3: ('goal_home', 'classification'),
            4: ('goal_away', 'classification'),
            5: ('total', 'classification'),
            6: ('total_home', 'classification'),
            7: ('total_away', 'classification'),
            8: ('total_amount', 'regression'),
            9: ('total_home_amount', 'regression'),
            10: ('total_away_amount', 'regression')
        }
    
    def migrate_outcomes(self) -> Dict[str, int]:
        """–ú–∏–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ outcomes –≤ statistics_optimized."""
        self.logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é outcomes...")
        
        with Session_pool() as db_session:
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ outcomes
            total_outcomes = db_session.query(Outcome).count()
            self.logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {total_outcomes} outcomes –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ outcomes —Å —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            outcomes = (
                db_session.query(Outcome)
                .join(Match, Outcome.match_id == Match.id)
                .join(ChampionShip, Match.tournament_id == ChampionShip.id)
                .join(Sport, ChampionShip.sport_id == Sport.id)
                .order_by(Outcome.id)
                .all()
            )
            
            migrated_count = 0
            error_count = 0
            skipped_count = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
            for i in tqdm(range(0, len(outcomes), self.batch_size), desc="–ú–∏–≥—Ä–∞—Ü–∏—è outcomes"):
                batch = outcomes[i:i + self.batch_size]
                
                for outcome in batch:
                    try:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ
                        existing = (
                            db_session.query(StatisticsOptimized)
                            .filter(StatisticsOptimized.outcome_id == outcome.id)
                            .first()
                        )
                        
                        if existing:
                            skipped_count += 1
                            continue
                        
                        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        statistics = self._create_statistics_from_outcome(outcome, db_session)
                        if statistics:
                            db_session.add(statistics)
                            migrated_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"–û—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏ outcome {outcome.id}: {e}")
                        error_count += 1
                        continue
                
                # –ö–æ–º–º–∏—Ç–∏–º –±–∞—Ç—á
                try:
                    db_session.commit()
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–º–º–∏—Ç–∞ –±–∞—Ç—á–∞ {i//self.batch_size + 1}: {e}")
                    db_session.rollback()
                    error_count += len(batch)
            
            result = {
                'total': total_outcomes,
                'migrated': migrated_count,
                'errors': error_count,
                'skipped': skipped_count
            }
            
            self.logger.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è outcomes –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")
            return result
    
    def migrate_predictions(self) -> Dict[str, int]:
        """–ú–∏–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ predictions –≤ statistics_optimized."""
        self.logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é predictions...")
        
        with Session_pool() as db_session:
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ predictions
            total_predictions = db_session.query(Prediction).count()
            self.logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {total_predictions} predictions –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ predictions —Å —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            predictions = (
                db_session.query(Prediction)
                .join(Match, Prediction.match_id == Match.id)
                .join(ChampionShip, Match.tournament_id == ChampionShip.id)
                .join(Sport, ChampionShip.sport_id == Sport.id)
                .order_by(Prediction.id)
                .all()
            )
            
            migrated_count = 0
            error_count = 0
            skipped_count = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
            for i in tqdm(range(0, len(predictions), self.batch_size), desc="–ú–∏–≥—Ä–∞—Ü–∏—è predictions"):
                batch = predictions[i:i + self.batch_size]
                
                for prediction in batch:
                    try:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ
                        existing = (
                            db_session.query(StatisticsOptimized)
                            .filter(StatisticsOptimized.prediction_id == prediction.id)
                            .first()
                        )
                        
                        if existing:
                            skipped_count += 1
                            continue
                        
                        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        statistics = self._create_statistics_from_prediction(prediction, db_session)
                        if statistics:
                            db_session.add(statistics)
                            migrated_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"–û—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏ prediction {prediction.id}: {e}")
                        error_count += 1
                        continue
                
                # –ö–æ–º–º–∏—Ç–∏–º –±–∞—Ç—á
                try:
                    db_session.commit()
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–º–º–∏—Ç–∞ –±–∞—Ç—á–∞ {i//self.batch_size + 1}: {e}")
                    db_session.rollback()
                    error_count += len(batch)
            
            result = {
                'total': total_predictions,
                'migrated': migrated_count,
                'errors': error_count,
                'skipped': skipped_count
            }
            
            self.logger.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è predictions –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")
            return result
    
    def _create_statistics_from_outcome(self, outcome: Outcome, db_session) -> StatisticsOptimized:
        """–°–æ–∑–¥–∞–µ—Ç –∑–∞–ø–∏—Å—å StatisticsOptimized –∏–∑ Outcome."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            match = db_session.query(Match).filter(Match.id == outcome.match_id).first()
            if not match:
                return None
            
            championship = db_session.query(ChampionShip).filter(ChampionShip.id == match.tournament_id).first()
            if not championship:
                return None
            
            sport = db_session.query(Sport).filter(Sport.id == championship.sport_id).first()
            if not sport:
                return None
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–æ–≥–Ω–æ–∑–∞
            forecast_type, model_type = self.feature_mapping.get(outcome.feature, ('unknown', 'unknown'))
            forecast_subtype = outcome.outcome or 'unknown'
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á–∞
            actual_result, actual_value = self._calculate_actual_result(match)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            statistics = StatisticsOptimized(
                outcome_id=outcome.id,
                prediction_id=None,
                match_id=outcome.match_id,
                championship_id=match.tournament_id,
                sport_id=championship.sport_id,
                match_date=match.gameData.date() if match.gameData else datetime.now().date(),
                match_round=getattr(match, 'tour', None),
                match_stage=getattr(match, 'stage', None),
                forecast_type=forecast_type,
                forecast_subtype=forecast_subtype,
                model_name='conformal_predictor',
                model_version='1.0',
                model_type=model_type,
                actual_result=actual_result,
                actual_value=actual_value,
                prediction_correct=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                prediction_accuracy=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                prediction_error=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                prediction_residual=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                coefficient=None,
                potential_profit=None,
                actual_profit=None
            )
            
            return statistics
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ outcome {outcome.id}: {e}")
            return None
    
    def _create_statistics_from_prediction(self, prediction: Prediction, db_session) -> StatisticsOptimized:
        """–°–æ–∑–¥–∞–µ—Ç –∑–∞–ø–∏—Å—å StatisticsOptimized –∏–∑ Prediction."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            match = db_session.query(Match).filter(Match.id == prediction.match_id).first()
            if not match:
                return None
            
            championship = db_session.query(ChampionShip).filter(ChampionShip.id == match.tournament_id).first()
            if not championship:
                return None
            
            sport = db_session.query(Sport).filter(Sport.id == championship.sport_id).first()
            if not sport:
                return None
            
            # –î–ª—è predictions —Å–ª–æ–∂–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π
            forecast_type = 'prediction'
            forecast_subtype = 'general'
            model_type = 'classification'
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á–∞
            actual_result, actual_value = self._calculate_actual_result(match)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            statistics = StatisticsOptimized(
                outcome_id=None,
                prediction_id=prediction.id,
                match_id=prediction.match_id,
                championship_id=match.tournament_id,
                sport_id=championship.sport_id,
                match_date=match.gameData.date() if match.gameData else datetime.now().date(),
                match_round=getattr(match, 'tour', None),
                match_stage=getattr(match, 'stage', None),
                forecast_type=forecast_type,
                forecast_subtype=forecast_subtype,
                model_name=prediction.model_name or 'keras_model',
                model_version='1.0',
                model_type=model_type,
                actual_result=actual_result,
                actual_value=actual_value,
                prediction_correct=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                prediction_accuracy=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                prediction_error=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                prediction_residual=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                coefficient=None,
                potential_profit=None,
                actual_profit=None
            )
            
            return statistics
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ prediction {prediction.id}: {e}")
            return None
    
    def _calculate_actual_result(self, match: Match) -> Tuple[str, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á–∞."""
        if not match.numOfHeadsHome or not match.numOfHeadsAway:
            return None, None
        
        goal_home = int(match.numOfHeadsHome)
        goal_away = int(match.numOfHeadsAway)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á–∞
        if goal_home > goal_away:
            actual_result = 'home_win'
        elif goal_home < goal_away:
            actual_result = 'away_win'
        else:
            actual_result = 'draw'
        
        # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (total_amount) –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—É–º–º—É –≥–æ–ª–æ–≤
        actual_value = float(goal_home + goal_away)
        
        return actual_result, actual_value
    
    def verify_migration(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∏–≥—Ä–∞—Ü–∏–∏."""
        self.logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∏–≥—Ä–∞—Ü–∏–∏...")
        
        with Session_pool() as db_session:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–ø–∏—Å–∏
            total_outcomes = db_session.query(Outcome).count()
            total_predictions = db_session.query(Prediction).count()
            total_statistics = db_session.query(StatisticsOptimized).count()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–∏
            statistics_with_outcomes = (
                db_session.query(StatisticsOptimized)
                .filter(StatisticsOptimized.outcome_id.isnot(None))
                .count()
            )
            
            statistics_with_predictions = (
                db_session.query(StatisticsOptimized)
                .filter(StatisticsOptimized.prediction_id.isnot(None))
                .count()
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            duplicate_outcomes = (
                db_session.query(StatisticsOptimized.outcome_id)
                .filter(StatisticsOptimized.outcome_id.isnot(None))
                .group_by(StatisticsOptimized.outcome_id)
                .having(db_session.func.count() > 1)
                .count()
            )
            
            duplicate_predictions = (
                db_session.query(StatisticsOptimized.prediction_id)
                .filter(StatisticsOptimized.prediction_id.isnot(None))
                .group_by(StatisticsOptimized.prediction_id)
                .having(db_session.func.count() > 1)
                .count()
            )
            
            result = {
                'total_outcomes': total_outcomes,
                'total_predictions': total_predictions,
                'total_statistics': total_statistics,
                'statistics_with_outcomes': statistics_with_outcomes,
                'statistics_with_predictions': statistics_with_predictions,
                'duplicate_outcomes': duplicate_outcomes,
                'duplicate_predictions': duplicate_predictions,
                'coverage_outcomes': (statistics_with_outcomes / total_outcomes * 100) if total_outcomes > 0 else 0,
                'coverage_predictions': (statistics_with_predictions / total_predictions * 100) if total_predictions > 0 else 0
            }
            
            self.logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏: {result}")
            return result


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏."""
    print("üöÄ –ú–ò–ì–†–ê–¶–ò–Ø –î–ê–ù–ù–´–• –í STATISTICS_OPTIMIZED")
    print("=" * 50)
    
    migrator = StatisticsDataMigrator(batch_size=1000)
    
    try:
        # 1. –ú–∏–≥—Ä–∏—Ä—É–µ–º outcomes
        print("\\nüìä –≠—Ç–∞–ø 1: –ú–∏–≥—Ä–∞—Ü–∏—è outcomes...")
        outcomes_result = migrator.migrate_outcomes()
        
        # 2. –ú–∏–≥—Ä–∏—Ä—É–µ–º predictions
        print("\\nüìä –≠—Ç–∞–ø 2: –ú–∏–≥—Ä–∞—Ü–∏—è predictions...")
        predictions_result = migrator.migrate_predictions()
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\\nüîç –≠—Ç–∞–ø 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        verification_result = migrator.verify_migration()
        
        # 4. –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        print("\\n" + "=" * 50)
        print("üìà –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ú–ò–ì–†–ê–¶–ò–ò")
        print("=" * 50)
        
        print(f"\\nüìä OUTCOMES:")
        print(f"   –í—Å–µ–≥–æ: {outcomes_result['total']}")
        print(f"   –ú–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ: {outcomes_result['migrated']}")
        print(f"   –û—à–∏–±–æ–∫: {outcomes_result['errors']}")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {outcomes_result['skipped']}")
        
        print(f"\\nüìä PREDICTIONS:")
        print(f"   –í—Å–µ–≥–æ: {predictions_result['total']}")
        print(f"   –ú–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ: {predictions_result['migrated']}")
        print(f"   –û—à–∏–±–æ–∫: {predictions_result['errors']}")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {predictions_result['skipped']}")
        
        print(f"\\nüìä STATISTICS_OPTIMIZED:")
        print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {verification_result['total_statistics']}")
        print(f"   –° outcomes: {verification_result['statistics_with_outcomes']}")
        print(f"   –° predictions: {verification_result['statistics_with_predictions']}")
        print(f"   –ü–æ–∫—Ä—ã—Ç–∏–µ outcomes: {verification_result['coverage_outcomes']:.1f}%")
        print(f"   –ü–æ–∫—Ä—ã—Ç–∏–µ predictions: {verification_result['coverage_predictions']:.1f}%")
        
        if verification_result['duplicate_outcomes'] > 0:
            print(f"   ‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç—ã outcomes: {verification_result['duplicate_outcomes']}")
        
        if verification_result['duplicate_predictions'] > 0:
            print(f"   ‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç—ã predictions: {verification_result['duplicate_predictions']}")
        
        print("\\n‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏: {e}")
        print(f"\\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
