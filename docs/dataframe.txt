Выбросы: https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-a-pandas-dataframe
Очистка данных: https://proglib.io/p/moem-dataset-rukovodstvo-po-ochistke-dannyh-v-python-2020-03-27?ysclid=m551av6zru94624974
Отрисовка выбрасов: https://habr.com/ru/articles/251225/
===
Что такое выброс?
Представьте себе ряд значений [3, 2, 3, 4, 999] (где 999 явно не вписывается) и проанализируйте различные способы выявления выбросов

Z-Оценка
Проблема в том, что рассматриваемое значение сильно искажает наши показатели mean и std, что приводит к ошибочным z-оценкам, примерно равным 
[-0.5, -0.5, -0.5, -0.5, 2.0], при этом каждое значение находится в пределах двух стандартных отклонений от среднего. 
Таким образом, один очень большой выброс может исказить всю вашу оценку выбросов. Я бы не рекомендовал использовать такой подход.

Квантильный фильтр
Более надёжный подход описан в этом ответе, где исключается нижний и верхний 1% данных. Однако это исключает фиксированную долю данных независимо от того, 
действительно ли эти данные являются выбросами. Вы можете потерять много достоверных данных, но, с другой стороны, сохранить некоторые выбросы, если 
у вас более 1% или 2% данных являются выбросами.

IQR-расстояние от Медианы
Ещё более надёжная версия квантильного принципа: исключите все данные, которые более чем в f раз отличаются от интерквартильного размахамедианы данных. 
Это также преобразование, которое, например, sklearn использует в RobustScaler's. Интерквартильный размах и медиана устойчивы к выбросам, поэтому 
вы решаете проблемы, связанные с подходом на основе z-показателей.

В нормальном распределении у нас есть примерно iqr=1.35*s, поэтому вы можете преобразовать z=3 фильтра z-показателей в f=2.22 фильтра iqr. Это позволит 
убрать 999 в приведённом выше примере.

Основное предположение состоит в том, что по крайней мере «средняя половина» ваших данных является достоверной и хорошо соответствует распределению, 
в то время как вы можете ошибиться, если у вашего распределения широкие «хвосты» и узкий интервал от q_25% до q_75%.

Передовые статистические методы
Конечно, существуют сложные математические методы, такие как критерий Пирса, тест Граббса или Q-тест Диксона, и это лишь некоторые из них, 
которые также подходят для данных с ненормальным распределением. Ни один из них не является простым в реализации, поэтому мы не будем рассматривать их далее.

Код
Замена всех выбросов во всех числовых столбцах на np.nan в примере фрейма данных. Этот метод устойчив к всем типам данных, которые предоставляет pandas, 
и может быть легко применён к фреймам данных со смешанными типами:

import pandas as pd
import numpy as np                                     

# sample data of all dtypes in pandas (column 'a' has an outlier)         # dtype:
df = pd.DataFrame({'a': list(np.random.rand(8)) + [123456, np.nan],       # float64
                   'b': [0,1,2,3,np.nan,5,6,np.nan,8,9],                  # int64
                   'c': [np.nan] + list("qwertzuio"),                     # object
                   'd': [pd.to_datetime(_) for _ in range(10)],           # datetime64[ns]
                   'e': [pd.Timedelta(_) for _ in range(10)],             # timedelta[ns]
                   'f': [True] * 5 + [False] * 5,                         # bool
                   'g': pd.Series(list("abcbabbcaa"), dtype="category")}) # category
cols = df.select_dtypes('number').columns  # limits to a (float), b (int) and e (timedelta)
df_sub = df.loc[:, cols]


# OPTION 1: z-score filter: z-score < 3
lim = np.abs((df_sub - df_sub.mean()) / df_sub.std(ddof=0)) < 3

# OPTION 2: quantile filter: discard 1% upper / lower values
lim = np.logical_and(df_sub < df_sub.quantile(0.99, numeric_only=False),
                     df_sub > df_sub.quantile(0.01, numeric_only=False))

# OPTION 3: iqr filter: within 2.22 IQR (equiv. to z-score < 3)
iqr = df_sub.quantile(0.75, numeric_only=False) - df_sub.quantile(0.25, numeric_only=False)
lim = np.abs((df_sub - df_sub.median()) / iqr) < 2.22


# replace outliers with nan
df.loc[:, cols] = df_sub.where(lim, np.nan)

===
Удалить все строки, содержащие хотя бы одно nan-значение:

df.dropna(subset=cols, inplace=True) # drop rows with NaN in numerical columns
# or
df.dropna(inplace=True)  # drop rows with NaN in any column

